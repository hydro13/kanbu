# Wiki Implementation Roadmap & Status

> **Laatst bijgewerkt:** 2026-01-12
> **Huidige fase:** Fase 15 - Wiki Intelligence üÜï
> **Sub-fase:** 15.1 Provider Koppeling | 15.2 Semantic Search | 15.3 Ask the Wiki | 15.4 Enhanced Graphs | 15.5 Integration
> **Vorige fase:** Fase 14 - AI Provider Configuration ‚úÖ COMPLEET
> **Volgende actie:** Start met 15.1 Provider Koppeling (Fase 14 ‚Üí Graphiti bridge)

---

## Fase 0: Foundation ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| WikiPage model (project) | ‚úÖ | schema.prisma |
| WorkspaceWikiPage model | ‚úÖ | schema.prisma |
| ProjectWikiVersion model | ‚úÖ | Voor version history |
| WorkspaceWikiVersion model | ‚úÖ | Voor version history |
| WikiPageStatus enum | ‚úÖ | DRAFT, PUBLISHED, ARCHIVED |
| Graphiti sync velden | ‚úÖ | graphitiGroupId, graphitiSynced, graphitiSyncedAt |
| projectWiki.ts router | ‚úÖ | Full CRUD + versions |
| workspaceWiki.ts router | ‚úÖ | Full CRUD + versions |
| wiki.permissions.ts | ‚úÖ | view, create, edit, delete, publish, history |

---

## Fase 1: Editor Integration ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| WikiSidebar.tsx | ‚úÖ | Tree navigation |
| WikiPageView.tsx | ‚úÖ | View/edit met Lexical |
| WikiVersionHistory.tsx | ‚úÖ | Version modal |
| WikiLinkPlugin.tsx | ‚úÖ | [[wiki links]] in editor |
| WikiLinkNode.tsx | ‚úÖ | Lexical node |
| WorkspaceWikiPage.tsx | ‚úÖ | Volledige pagina |
| Routes in App.tsx | ‚úÖ | /workspace/:slug/wiki/* |
| MarkdownPastePlugin | ‚úÖ | Showdown + tables support |

---

## Fase 2: Graphiti Integration ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| Graphiti/FalkorDB opzetten | ‚úÖ | Port 6379 (redis), 3000 (UI) |
| GraphitiService class | ‚úÖ | apps/api/src/services/graphitiService.ts |
| Sync on wiki save | ‚úÖ | Hooks in create/update/delete mutations |
| Entity extraction | ‚úÖ | Rules-based (@mentions, #tasks, concepts) |
| graphiti.getBacklinks endpoint | ‚úÖ | graphiti.ts router |
| graphiti.search endpoint | ‚úÖ | graphiti.ts router |
| graphiti.getRelated endpoint | ‚úÖ | graphiti.ts router |

### Vereisten voor Fase 2:
- [x] FalkorDB draaiend op MAX (kanbu-falkordb container)
- [ ] LLM-based entity extraction (future improvement)
- [x] Sync hooks in wiki routers
- [x] tRPC endpoints voor graph queries

---

## Fase 3: Cross-References ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| BacklinksPanel component | ‚úÖ | components/wiki/BacklinksPanel.tsx |
| Related pages in panel | ‚úÖ | Ge√Øntegreerd in BacklinksPanel |
| Integratie in WikiPageView | ‚úÖ | Toont panel onder content |
| @mentions plugin | ‚úÖ | MentionPlugin.tsx, MentionNode.tsx |
| &Sign plugin | ‚úÖ | SignaturePlugin.tsx, SignatureNode.tsx (DecoratorNode) |
| #task-refs plugin | ‚úÖ | TaskRefPlugin.tsx, TaskRefNode.tsx |

---

## Fase 4: Search & Discovery üîÑ IN PROGRESS

| Item | Status | Notities |
|------|--------|----------|
| Text search (graph) | ‚úÖ | Cypher CONTAINS query op titles/entities |
| Wiki search UI | ‚úÖ | WikiSearchDialog.tsx met keyboard nav |
| Cmd+K wiki search | ‚úÖ | Wiki pages zoeken via CommandPalette |
| Semantic search (vectors) | ‚ùå | Vereist embeddings + Qdrant (toekomst) |

---

## Fase 5: Graph Visualization üîÑ IN PROGRESS

| Item | Status | Notities |
|------|--------|----------|
| D3.js installatie | ‚úÖ | d3 + @types/d3 |
| getGraph endpoint | ‚úÖ | graphiti.ts + graphitiService.ts |
| WikiGraphView component | ‚úÖ | Force-directed layout, zoom/pan |
| Sidebar toggle button | ‚úÖ | Network icon in WikiSidebar |
| Fullscreen mode | ‚úÖ | Uitklapbaar naar volledig scherm |
| 3D/WebXR support | ‚ùå | Three.js integratie (toekomst) |
| 100k+ nodes | ‚ùå | WebGPU/Cosmos integratie (toekomst) |

---

---

# GRAPHITI CORE INTEGRATIE

> **Doel:** Volledige Graphiti Python library integreren in Kanbu voor maximale controle en aanpasbaarheid.
> **Bron:** https://github.com/getzep/graphiti (geforkt naar apps/graphiti/)

---

## Fase 7: Python Service Setup ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| apps/graphiti/ directory aanmaken | ‚úÖ | Nieuwe app in monorepo |
| graphiti_core code kopi√´ren | ‚úÖ | Van ~/repos/graphiti/ naar src/core/ |
| pyproject.toml + dependencies | ‚úÖ | uv package manager |
| FastAPI wrapper service | ‚úÖ | src/api/main.py + schemas.py |
| Dockerfile voor graphiti service | ‚úÖ | Python 3.11-slim image |
| docker-compose.yml updaten | ‚úÖ | graphiti service op poort 8000 |
| .env configuratie | ‚úÖ | .env.example aangemaakt |
| Health check endpoint | ‚úÖ | GET /health endpoint

---

## Fase 8: Kanbu API Integratie ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| GraphitiClient class in Node.js | ‚úÖ | lib/graphitiClient.ts met typed methods |
| graphitiService.ts refactoren | ‚úÖ | Python service first, FalkorDB fallback |
| Episode sync bij wiki save | ‚úÖ | add_episode via HTTP met fallback |
| Error handling + retries | ‚úÖ | GraphitiClientError, timeout, graceful degradation |
| Connection pooling | ‚úÖ | Native fetch, 60s health check cache |

---

## Fase 9: Bi-Temporal Model üîÑ IN PROGRESS

| Item | Status | Notities |
|------|--------|----------|
| valid_at / invalid_at velden | ‚úÖ | graphiti_core heeft native support |
| created_at / expired_at tracking | ‚ùå | Audit trail |
| Temporal query endpoints | ‚úÖ | temporalSearch in graphiti.ts + Python service |
| Version diff met temporal context | ‚úÖ | WikiTemporalSearch.tsx component |
| Contradiction detection | ‚ùå | LLM detecteert conflicten |

---

## Fase 10: LLM Entity Extraction ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| LLM provider configuratie | ‚úÖ | OpenAI via graphiti_core (al ge√Øntegreerd in Fase 7/8) |
| Entity extraction pipeline | ‚úÖ | graphiti_core add_episode met custom entity_types |
| Custom entity types | ‚úÖ | WikiPage, Task, User, Project, Concept in kanbu_entities.py |
| Relation extraction | ‚úÖ | Native in graphiti_core - automatische relatie-extractie via LLM |
| Concept deduplicatie | ‚úÖ | Native in graphiti_core - dedupe_nodes.py prompts |

**Notitie:** Graphiti_core heeft built-in LLM-based:
- **Entity extraction** met custom types (via `entity_types` parameter)
- **Relation extraction** (automatisch bij `add_episode()`)
- **Entity deduplication** (via dedupe_nodes prompts)

Alle functies zijn nu actief wanneer de Python Graphiti service draait met OPENAI_API_KEY.

---

## Fase 11: Embeddings & Semantic Search ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| Embedding provider setup | ‚úÖ | OpenAI text-embedding-3-small (configurable via env) |
| fact_embedding generatie | ‚úÖ | Automatisch bij add_episode via graphiti_core |
| Vector storage | ‚úÖ | FalkorDB (native in graphiti_core, niet Qdrant) |
| Hybrid search (BM25 + vector) | ‚úÖ | POST /search/hybrid endpoint met configurable methods |
| Search ranking/reranking | ‚úÖ | RRF, MMR, Cross-encoder reranking support |

**Notitie:** Graphiti_core slaat embeddings op in FalkorDB zelf (als node/edge properties), niet in een externe vector DB zoals Qdrant.

**Endpoints:**
- `POST /search/hybrid` - Hybrid search met BM25 + vector similarity + BFS
- `GET /health` - Nu met embedding_model en embedding_dim info

**Environment variabelen:**
- `EMBEDDING_MODEL` (default: text-embedding-3-small)
- `EMBEDDING_DIM` (default: 1024)

---

## Fase 12: MCP Server & Claude Integratie

| Item | Status | Notities |
|------|--------|----------|
| MCP protocol endpoints | ‚ùå | add_memory, search_nodes, etc. |
| Claude Desktop integratie | ‚ùå | Persistent memory |
| Agent memory per workspace | ‚ùå | group_id isolatie |
| "Ask the Wiki" chatbox | ‚ùå | RAG over wiki content |

---

## Fase 13: Advanced Features

| Item | Status | Notities |
|------|--------|----------|
| Queue-based processing | ‚ùå | Concurrent editing support |
| Auto-suggestions tijdens typen | ‚ùå | Real-time entity hints |
| Graph analytics dashboard | ‚ùå | Statistieken, trends |
| Export/import graph data | ‚ùå | Backup/restore |
| Multi-tenant graph isolation | ‚ùå | Workspace boundaries |

---

## Fase 14: AI Provider Configuration üÜï

> **Doel:** Multi-environment deployment ondersteuning met configureerbare AI providers op 3 niveaus.
> **Scope:** Van laptop/offline tot enterprise SaaS met volledige provider keuze.

### Waarom Fase 14?

Kanbu moet werken in verschillende deployment scenarios:
- **Laptop/Offline:** Ollama met lokale modellen (privacy-first)
- **On-premise:** Eigen LLM servers achter firewall
- **SaaS:** Managed service met OpenAI/Anthropic
- **Enterprise:** ChatLLM Teams (Abacus.ai) integratie

### Sub-fases

#### 14.0 Research: Providers & Hardware

##### 14.0.1 Ollama: Hardware & Model Configuratie ‚úÖ COMPLEET

| Item | Status | Notities |
|------|--------|----------|
| Hardware tier definitie | ‚úÖ | 7 tiers: CPU-only, Entry, Mid, High, Pro, Apple, Ultra |
| VRAM requirements per model | ‚úÖ | Complete matrix Q4/Q5/Q8/FP16 |
| Quantization impact analyse | ‚úÖ | Q4_K_M = best balance, FP16 = 4x meer VRAM |
| Model aanbevelingen per tier | ‚úÖ | Per tier model combos (LLM + embed + context) |
| Vision model haalbaarheid | ‚úÖ | Vanaf 8GB VRAM (llava:7b) |
| CPU-only fallback strategie | ‚úÖ | 3-6 tok/s, bruikbaar voor batch |
| Auto-detect hardware capability | ‚úÖ | Via Ollama API `/api/ps` en `/api/tags` |

**Status:** ‚úÖ COMPLEET - Zie [RESEARCH-Ollama-Hardware.md](RESEARCH-Ollama-Hardware.md)

**Key Findings:**

| Eigenschap | Waarde |
|------------|--------|
| Minimum VRAM | **8GB** (7B Q4 + embeddings) |
| Aanbevolen VRAM | **12-16GB** (13B + ruimte) |
| Default num_ctx | **2048** ‚ö†Ô∏è TE KLEIN! |
| NPU Support | ‚ùå Niet in Ollama |

**Hardware Tiers (definitief):**

| Tier | VRAM | LLM | Embed | Vision | Performance |
|------|------|-----|-------|--------|-------------|
| CPU-only | 16GB RAM | 7B Q4 | ‚úÖ | ‚ùå | 3-6 tok/s |
| Entry | 6-8 GB | 7B Q4 | ‚úÖ | ‚ùå | 40+ tok/s |
| **Mid** | 10-12 GB | 13B Q4 | ‚úÖ | ‚ö†Ô∏è | 35-50 tok/s |
| High | 16-24 GB | 30B Q4 | ‚úÖ | ‚úÖ | 30-45 tok/s |
| Pro | 48+ GB | 70B Q4 | ‚úÖ | ‚úÖ | Full speed |
| Apple M1-M3 | 8-64GB | 7-13B | ‚úÖ | ‚ö†Ô∏è | 15-45 tok/s |
| Apple Ultra | 64-192GB | 70B+ | ‚úÖ | ‚úÖ | 50+ tok/s |

**GPU Vendor Support:**

| Vendor | Status | Notes |
|--------|--------|-------|
| NVIDIA (CUDA) | ‚úÖ Best | GTX 900+, RTX, A-series |
| AMD (ROCm) | ‚úÖ Good | RX 6000+, via HSA_OVERRIDE voor older |
| Apple (Metal) | ‚úÖ Excellent | M1-M4 series, unified memory |
| Intel (Vulkan) | ‚ö†Ô∏è Experimental | Arc GPUs, OLLAMA_VULKAN=1 |
| NPU (all) | ‚ùå None | Niet in Ollama/llama.cpp |

**Kanbu Aanbevolen Configuratie:**

```bash
# KRITIEK: Verhoog context window!
export OLLAMA_CONTEXT_LENGTH=8192
export OLLAMA_FLASH_ATTENTION=1

# Model combinatie (Mid tier):
# LLM: llama3.2:8b (~5GB)
# Embed: nomic-embed-text (~0.5GB)
# Context 16K: +4GB
# Total: ~9.5GB
```

**NPU Conclusie:** AMD XDNA en Intel NPU zijn **niet bruikbaar** met Ollama. Voor NPU moet ONNX/OpenVINO stack gebruikt worden (out of scope voor v1).

---

##### 14.0.2 Abacus.ai / ChatLLM Teams

| Item | Status | Notities |
|------|--------|----------|
| ChatLLM Teams documentatie onderzoeken | ‚ùå | API capabilities, pricing model |
| Enterprise features inventariseren | ‚ùå | SSO, audit logs, compliance |
| LLM gateway functionaliteit | ‚ùå | Model routing, fallbacks |
| Embedding support onderzoeken | ‚ùå | Welke embedding modellen? |
| API authenticatie methode | ‚ùå | API keys, OAuth, SAML? |
| Kanbu integratie haalbaarheid | ‚ùå | Conclusie + recommendation |

**Deliverable:** Research document met conclusie over ChatLLM Teams integratie.

**Status:** ‚úÖ COMPLEET - Zie [RESEARCH-Abacus-AI-ChatLLM.md](RESEARCH-Abacus-AI-ChatLLM.md)

**Conclusie:** Niet aanbevolen als primaire provider. Geen embedding API zonder Enterprise tier ($5K+/maand).

---

##### 14.0.3 OpenCode: Open Source AI Coding Agent ‚úÖ COMPLEET

> **Bron:** [opencode.ai](https://opencode.ai/) | [GitHub](https://github.com/opencode-ai/opencode)

| Item | Status | Notities |
|------|--------|----------|
| OpenCode architectuur onderzoeken | ‚úÖ | Go-based CLI, TUI, client/server, 50K+ stars |
| Multi-provider support analyseren | ‚úÖ | 75+ LLM providers via AI SDK + Models.dev |
| Self-hosted endpoint support | ‚úÖ | @ai-sdk/openai-compatible, Ollama (num_ctx fix!) |
| OpenCode Zen evalueren | ‚úÖ | Pay-as-you-go, free tier incl. GLM-4.7 |
| GitHub Actions integratie | ‚úÖ | /opencode mentions, auto PR creation |
| Kanbu integratie haalbaarheid | ‚úÖ | Inspiratie ja, directe integratie nee |

**Status:** ‚úÖ COMPLEET - Zie [RESEARCH-OpenCode.md](RESEARCH-OpenCode.md)

**Key Findings:**

| Eigenschap | Waarde |
|------------|--------|
| GitHub Stars | 50.000+ |
| Monthly Users | 650.000+ |
| Providers | 75+ via AI SDK |
| Ollama Support | ‚úÖ (maar num_ctx fix nodig!) |
| Embedding Support | ‚ùå **Geen** |
| Web UI | ‚ùå CLI/TUI only |

**OpenCode Zen Pricing:**

| Tier | Voorbeelden | Per 1M tokens |
|------|-------------|---------------|
| Free | GLM-4.7, GPT 5 Nano, Grok Code | $0 |
| Budget | Claude Haiku, Gemini Flash | $0.50-$1 / $3-$5 |
| Premium | Claude Opus, GPT 5.2 | $1.75-$5 / $14-$25 |

**Kritieke Ollama Fix:**
```bash
# Default context window is 4096 - TE KLEIN!
ollama run <model>
/set parameter num_ctx 32768
/save <model>
```

**Conclusie:**
- ‚úÖ **Waardevolle inspiratie** voor provider abstractie patroon
- ‚úÖ **Ollama config lesson** learned (num_ctx!)
- ‚ùå **Niet voor directe integratie** (geen embeddings, CLI-only)

---

##### 14.0.4 GLM-4.7: Chinese Open Source Coding Model ‚úÖ COMPLEET

> **Bron:** [Z.ai GLM-4.7 Blog](https://z.ai/blog/glm-4.7) | [Hugging Face](https://huggingface.co/zai-org/GLM-4.7)

| Item | Status | Notities |
|------|--------|----------|
| GLM-4.7 capabilities onderzoeken | ‚úÖ | 358B MoE, 200K context, #1 SWE-bench |
| Model varianten inventariseren | ‚úÖ | Alleen 358B (geen 9B/32B varianten!) |
| API toegang evalueren | ‚úÖ | OpenAI-compatible, $0.40/$1.50 per 1M |
| Local deployment onderzoeken | ‚úÖ | Ollama/vLLM/SGLang, 135-205GB RAM vereist |
| Benchmark vergelijking | ‚úÖ | #1 open-source coding model |
| Kanbu integratie haalbaarheid | ‚úÖ | ‚úÖ CODE via API, ‚ùå self-hosted |

**Status:** ‚úÖ COMPLEET - Zie [RESEARCH-GLM-4.7.md](RESEARCH-GLM-4.7.md)

**Key Findings:**

| Specificatie | Waarde |
|--------------|--------|
| Parameters | **358B MoE** (geen kleinere varianten!) |
| Context Window | 200.000 tokens |
| Max Output | 128.000 tokens |
| API Prijs | $0.40/$1.50 per 1M tokens |
| Coding Plan | $3/maand |
| Self-hosted | 135-205GB RAM vereist |

**Benchmarks (december 2025):**

| Benchmark | Score | Opmerking |
|-----------|-------|-----------|
| SWE-bench Verified | **73.8%** | #1 open-source model |
| LiveCodeBench | **84.9%** | > Claude Sonnet 4.5 |
| AIME 2025 (math) | 95.7% | Zeer sterke reasoning |

**Embedding Support:**
- Z.ai biedt `embedding-3` model (configureerbare dimensies)
- LangChain integratie beschikbaar

**Conclusie:**
- ‚úÖ **Aanbevolen als CODE capability provider** via API of OpenRouter
- ‚ùå **Niet aanbevolen voor self-hosted** (hardware te zwaar)
- ‚ö†Ô∏è **GDPR:** Chinese servers, onduidelijk privacy beleid

---

##### 14.0.5 LM Studio: Local Model Server ‚úÖ COMPLEET

> **Bron:** [lmstudio.ai](https://lmstudio.ai/) | [GitHub](https://github.com/lmstudio-ai/lms)

| Item | Status | Notities |
|------|--------|----------|
| LM Studio architectuur onderzoeken | ‚úÖ | Desktop app + CLI (`lms`), llama.cpp + MLX engines |
| OpenAI-compatible API analyseren | ‚úÖ | /v1/chat/completions, /v1/embeddings, /v1/responses |
| Model management features | ‚úÖ | GGUF + MLX, HuggingFace direct, JIT loading |
| Hardware acceleration support | ‚úÖ | CUDA (incl. RTX 50), Metal/MLX, Vulkan, CPU |
| Multi-model serving capability | ‚úÖ | Model switching (geen concurrent serving) |
| Vergelijking met Ollama | ‚úÖ | Ollama 20% sneller, LM Studio beter op integrated GPU |
| Kanbu integratie haalbaarheid | ‚úÖ | Optioneel alternatief voor desktop users |

**Status:** ‚úÖ COMPLEET - Zie [RESEARCH-LM-Studio.md](RESEARCH-LM-Studio.md)

**Key Findings:**

| Eigenschap | Waarde |
|------------|--------|
| Licentie | Gratis (closed source) |
| Huidige versie | 0.3.37 |
| Default Port | **1234** (vs Ollama 11434) |
| API | OpenAI-compatible (`/v1/*`) |
| Embeddings | ‚úÖ nomic-embed-text, bge-small |
| Headless | ‚ö†Ô∏è Vereist GUI support op systeem |
| Docker | ‚ùå Geen official image |

**Performance vs Ollama (M3 Max):**

| Metric | Ollama | LM Studio |
|--------|--------|-----------|
| Cold Start | 3.2s | 8.7s |
| Tokens/sec | 85.2 | 72.8 |
| Memory | 4.2GB | 5.8GB |

**Unieke Voordelen LM Studio:**
- ‚úÖ **Vulkan support** - Werkt op integrated GPUs (Intel/AMD)
- ‚úÖ **MLX engine** - Apple Silicon geoptimaliseerd
- ‚úÖ **GUI** - Intu√Øtief voor eindgebruikers
- ‚úÖ **Model browser** - 1000+ pre-configured modellen

**Beperkingen:**
- ‚ùå Geen echte headless mode (vereist X11/Wayland)
- ‚ùå Geen Docker container
- ‚ùå Closed source
- ‚ö†Ô∏è Tool calling experimenteel

**Conclusie:**
- ‚úÖ **Optioneel alternatief** voor desktop/GUI users
- ‚úÖ **Aanbevolen voor integrated GPU** users
- ‚ùå **Niet voor server deployments** - gebruik Ollama
- ‚úÖ **Zelfde API** - kan via @ai-sdk/openai-compatible

---

#### 14.1 Database Model: AiProviderConfig

> **BESLUIT (2026-01-12):** Na research van 5 providers (Ollama, Abacus.ai, OpenCode, GLM-4.7, LM Studio) is besloten om slechts 3 providers te ondersteunen voor de Wiki/Graphiti implementatie.
>
> **Focus:** Embeddings, Reasoning, Vision - GEEN code generation (niet relevant voor Wiki).
>
> **Afgevallen:**
> - Anthropic: Geen embedding API
> - Abacus.ai: Embeddings alleen in Enterprise tier ($5K+/maand)
> - GLM-4.7: Alleen code specialist, overkill voor Wiki
> - OpenCode: Inspiratie, geen directe integratie
> - CUSTOM: Complexiteit niet nodig in v1

```prisma
// Nieuwe modellen in schema.prisma

enum AiProviderType {
  OPENAI       // Cloud - volledig (embeddings + reasoning + vision)
  OLLAMA       // Local - primair
  LM_STUDIO    // Local - optioneel (GUI/desktop users)
}

enum AiCapability {
  EMBEDDING    // Vector embeddings (Wiki search)
  REASONING    // Entity extraction, summarization (Graphiti)
  VISION       // Image understanding (optioneel, toekomstig)
}

model AiProviderConfig {
  id              Int             @id @default(autoincrement())

  // Scope (alleen √©√©n is gezet)
  isGlobal        Boolean         @default(false)
  workspaceId     Int?
  projectId       Int?

  // Provider settings
  providerType    AiProviderType  // OPENAI, OLLAMA, LM_STUDIO
  name            String          // Display name (bijv. "Production OpenAI")
  isActive        Boolean         @default(true)
  priority        Int             @default(0)  // Voor fallback ordering (0 = hoogste)

  // Capabilities die deze provider ondersteunt
  capabilities    AiCapability[]  // EMBEDDING, REASONING, VISION

  // Connection settings (encrypted)
  baseUrl         String?         // Endpoint URL
                                  // OpenAI: https://api.openai.com/v1 (default)
                                  // Ollama: http://localhost:11434
                                  // LM Studio: http://localhost:1234
  apiKey          String?         // Encrypted API key (alleen OpenAI)
  organizationId  String?         // OpenAI organization ID (optioneel)

  // Model preferences per capability
  embeddingModel  String?         // bijv. "text-embedding-3-small", "nomic-embed-text"
  reasoningModel  String?         // bijv. "gpt-4o-mini", "llama3.2:8b"
  visionModel     String?         // bijv. "gpt-4o", "llava:7b" (optioneel)

  // Rate limiting (alleen relevant voor cloud providers)
  maxRequestsPerMinute  Int?
  maxTokensPerMinute    Int?

  // Metadata
  createdAt       DateTime        @default(now())
  updatedAt       DateTime        @updatedAt
  createdBy       Int?

  // Relations
  workspace       Workspace?      @relation(fields: [workspaceId], references: [id])
  project         Project?        @relation(fields: [projectId], references: [id])
  creator         User?           @relation(fields: [createdBy], references: [id])

  @@index([workspaceId])
  @@index([projectId])
  @@index([isGlobal])
  @@index([providerType])
}
```

| Item | Status | Notities |
|------|--------|----------|
| AiProviderConfig model | ‚úÖ | Prisma schema toegevoegd (2026-01-12) |
| AiProviderType enum | ‚úÖ | OPENAI, OLLAMA, LM_STUDIO (besluit 2026-01-12) |
| AiCapability enum | ‚úÖ | EMBEDDING, REASONING, VISION (geen CODE - niet voor Wiki) |
| Database migratie | ‚úÖ | `pnpm prisma db push` uitgevoerd (2026-01-12) |
| Seed data voor defaults | ‚úÖ | 3 global providers aangemaakt (2026-01-12) |

**Implementatie Details (2026-01-12):**
- Schema: `packages/shared/prisma/schema.prisma`
- Seed: `packages/shared/prisma/seed-ai-providers.ts`
- Providers: OpenAI (Global), Ollama (Local), LM Studio (Desktop)
- Alle providers staan standaard op `isActive: false` (handmatige configuratie vereist)

---

#### 14.2 Admin UI: System Settings > AI Systems

**Locatie:** Administration > System Settings > AI Systems (nieuw menu item)

> **Scope:** Alleen 3 providers (OpenAI, Ollama, LM Studio) en 3 capabilities (Embedding, Reasoning, Vision).

| Item | Status | Notities |
|------|--------|----------|
| AiSystemsPage.tsx | ‚úÖ | Main admin page met provider cards |
| AiProviderList.tsx | ‚úÖ | Ge√Øntegreerd in AiSystemsPage (ProviderSection) |
| AiProviderForm.tsx | ‚úÖ | ProviderModal + EditProviderModal in AiSystemsPage |
| AiProviderCard.tsx | ‚úÖ | ProviderCard component in AiSystemsPage |
| API key input met mask | ‚úÖ | Password input, backend maskeert met ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢  |
| Connection test button | ‚úÖ | Test /v1/models endpoint, toont latency + models |
| Model selector dropdown | ‚úÖ | getModels procedure haalt beschikbare modellen op |
| Ollama URL configuratie | ‚úÖ | Default: http://localhost:11434/v1 |
| LM Studio URL configuratie | ‚úÖ | Default: http://localhost:1234/v1 |

**UI Mockup:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Administration > System Settings > AI Systems                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ ‚îå‚îÄ Providers (3) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ ‚òÅÔ∏è OpenAI             ‚îÇ  ‚îÇ üñ•Ô∏è Ollama (Local)    ‚îÇ           ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ ‚úì Active (primair)   ‚îÇ  ‚îÇ ‚óã Not configured     ‚îÇ           ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ gpt-4o-mini          ‚îÇ  ‚îÇ                      ‚îÇ           ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ text-embed-3-small   ‚îÇ  ‚îÇ [Configure]          ‚îÇ           ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ [Edit] [Test]        ‚îÇ  ‚îÇ                      ‚îÇ           ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ üñ•Ô∏è LM Studio (Local)  ‚îÇ  ‚ìò Optioneel voor GUI users        ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ ‚óã Not configured     ‚îÇ  Alleen als Ollama niet werkt       ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ                      ‚îÇ  op integrated GPU                  ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ [Configure]          ‚îÇ                                     ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ ‚îå‚îÄ Capabilities Status ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îÇ Embedding:  OpenAI (text-embedding-3-small)   ‚úì Configured   ‚îÇ ‚îÇ
‚îÇ ‚îÇ Reasoning:  OpenAI (gpt-4o-mini)              ‚úì Configured   ‚îÇ ‚îÇ
‚îÇ ‚îÇ Vision:     Not configured                    ‚ö† Optional     ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚ìò Embedding en Reasoning zijn vereist voor Wiki/Graphiti.    ‚îÇ ‚îÇ
‚îÇ ‚îÇ   Vision is optioneel voor image understanding.              ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ ‚îå‚îÄ Quick Setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚òÅÔ∏è Cloud Setup (OpenAI)                                       ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ API Key: sk-‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢    [Show] [Test]‚îÇ  ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îÇ üñ•Ô∏è Local Setup (Ollama)                                       ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ URL: http://localhost:11434                       [Test]‚îÇ  ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ Status: ‚ö† Not running                                   ‚îÇ  ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îÇ [Start Guide] - Hoe Ollama installeren                  ‚îÇ  ‚îÇ ‚îÇ
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Provider Form Fields:**

| Provider | Velden |
|----------|--------|
| OpenAI | API Key (required), Organization ID (optional), Base URL (optional voor Azure) |
| Ollama | Base URL (default: http://localhost:11434), Model selection |
| LM Studio | Base URL (default: http://localhost:1234), Model selection |

---

#### 14.3 Provider Abstraction Layer

> **Scope:** 3 providers (OpenAI, Ollama, LM Studio), 3 capabilities (Embedding, Reasoning, Vision).
> **Aanpak:** OpenAI-compatible API voor alle providers (Ollama en LM Studio gebruiken zelfde interface).

```typescript
// lib/ai/providers/types.ts

export type AiProviderType = 'OPENAI' | 'OLLAMA' | 'LM_STUDIO'
export type AiCapability = 'EMBEDDING' | 'REASONING' | 'VISION'

export interface AiProvider {
  readonly type: AiProviderType
  readonly capabilities: AiCapability[]
  readonly baseUrl: string

  // Health check
  testConnection(): Promise<{ success: boolean; error?: string; models?: string[] }>

  // Model discovery
  listModels(capability?: AiCapability): Promise<string[]>
}

export interface EmbeddingProvider extends AiProvider {
  embed(text: string): Promise<number[]>
  embedBatch(texts: string[]): Promise<number[][]>
  getDimensions(): number
  getModelName(): string
}

export interface ReasoningProvider extends AiProvider {
  // Entity extraction, summarization voor Graphiti
  extractEntities(text: string, entityTypes: string[]): Promise<ExtractedEntity[]>
  summarize(text: string, maxLength?: number): Promise<string>

  // Generic chat (indien nodig)
  chat(messages: ChatMessage[], options?: ReasoningOptions): Promise<string>
  stream(messages: ChatMessage[], options?: ReasoningOptions): AsyncIterable<string>
}

export interface VisionProvider extends AiProvider {
  // Image understanding (optioneel)
  describeImage(image: Buffer | string, prompt?: string): Promise<string>
  extractTextFromImage(image: Buffer | string): Promise<string>
}

// Factory
export function createProvider(config: AiProviderConfig): AiProvider
export function createEmbeddingProvider(config: AiProviderConfig): EmbeddingProvider
export function createReasoningProvider(config: AiProviderConfig): ReasoningProvider
export function createVisionProvider(config: AiProviderConfig): VisionProvider | null
```

| Item | Status | Notities |
|------|--------|----------|
| AiProvider interface | ‚úÖ | Base interface in types.ts |
| EmbeddingProvider interface | ‚úÖ | embed(), embedBatch(), getDimensions() |
| ReasoningProvider interface | ‚úÖ | extractEntities(), summarize(), chat(), stream() |
| VisionProvider interface | ‚úÖ | describeImage(), extractTextFromImage() |
| **OpenAiProvider** | ‚úÖ | Volledige implementatie met SDK |
| **OllamaProvider** | ‚úÖ | OpenAI-compatible + native /api/tags fallback |
| **LmStudioProvider** | ‚úÖ | OpenAI-compatible op :1234/v1 |
| Provider factory | ‚úÖ | createProvider(), createEmbeddingProvider(), etc. |
| Provider registry | ‚úÖ | Singleton met scope resolution + caching |

**Provider Implementatie Details:**

```typescript
// lib/ai/providers/OpenAiProvider.ts
export class OpenAiProvider implements EmbeddingProvider, ReasoningProvider, VisionProvider {
  readonly type = 'OPENAI'
  readonly capabilities: AiCapability[] = ['EMBEDDING', 'REASONING', 'VISION']

  constructor(private config: { apiKey: string; baseUrl?: string }) {}

  // Gebruikt official OpenAI SDK
  // text-embedding-3-small voor embeddings
  // gpt-4o-mini voor reasoning
  // gpt-4o voor vision
}

// lib/ai/providers/OllamaProvider.ts
export class OllamaProvider implements EmbeddingProvider, ReasoningProvider, VisionProvider {
  readonly type = 'OLLAMA'
  readonly capabilities: AiCapability[] = ['EMBEDDING', 'REASONING', 'VISION']

  constructor(private config: { baseUrl: string }) {}

  // Gebruikt @ai-sdk/openai-compatible
  // nomic-embed-text voor embeddings
  // llama3.2:8b voor reasoning
  // llava:7b voor vision (indien beschikbaar)

  // KRITIEK: num_ctx moet 8192+ zijn!
}

// lib/ai/providers/LmStudioProvider.ts
export class LmStudioProvider implements EmbeddingProvider, ReasoningProvider, VisionProvider {
  readonly type = 'LM_STUDIO'
  readonly capabilities: AiCapability[] = ['EMBEDDING', 'REASONING', 'VISION']

  constructor(private config: { baseUrl: string }) {}

  // Zelfde interface als Ollama (OpenAI-compatible)
  // Alleen baseUrl verschilt (poort 1234 vs 11434)
}
```

**Belangrijke Notities:**

1. **Geen Anthropic/Abacus.ai:** Afgevallen wegens geen embeddings (zie 14.0 research).
2. **OpenAI-compatible:** Ollama en LM Studio gebruiken dezelfde API structuur als OpenAI.
3. **Ollama num_ctx:** Default 2048 is te klein! Moet naar 8192+ voor Graphiti context.
4. **Vision optioneel:** Niet alle installaties hebben vision modellen.

---

#### 14.4 Workspace & Project Overrides

> **Use Case:** Privacy-gevoelige workspaces kunnen lokale LLM (Ollama) forceren.
> **Use Case:** Offline development met Ollama/LM Studio als fallback.

| Item | Status | Notities |
|------|--------|----------|
| Workspace AI Settings page | ‚úÖ | WorkspaceAiConfigCard in WorkspaceSettings.tsx |
| Project AI Settings tab | ‚è∏Ô∏è | Deferred - workspace level is voldoende voor v1 |
| Inheritance logic | ‚úÖ | registry.ts findEffectiveConfig() |
| Override indicators in UI | ‚úÖ | Badge + icons (global/workspace) in WorkspaceAiConfigCard |
| getEffectiveProvider() service | ‚úÖ | workspaceAiProvider.getEffective/getEffectiveAll |
| Fallback logic | ‚úÖ | registry.ts getProviderWithFallback() |

**Implementatie Details (2026-01-12):**
- Backend: `apps/api/src/trpc/procedures/workspaceAiProvider.ts` - CRUD + getEffective
- Frontend: `apps/web/src/components/workspace/WorkspaceAiConfigCard.tsx`
- Registry: `apps/api/src/lib/ai/providers/registry.ts` - Scope resolution + caching

**Inheritance Regels:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Provider Resolution (per Capability)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Request: getEmbeddingProvider() voor Project "KANBU-123"        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  1. Check Project config [EMBEDDING] ‚Üí Not set                   ‚îÇ
‚îÇ  2. Check Workspace config [EMBEDDING] ‚Üí Ollama (override!)      ‚îÇ
‚îÇ  3. Check Global config [EMBEDDING] ‚Üí OpenAI (default)           ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Result: Use Ollama for embeddings in this project               ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Note: Reasoning en Vision kunnen andere providers hebben!       ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Fallback Chain:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Automatic Fallback                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Scenario: OpenAI API onbereikbaar                               ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  1. Try OpenAI          ‚Üí Error: API unavailable                 ‚îÇ
‚îÇ  2. Fallback to Ollama  ‚Üí Error: Not running                     ‚îÇ
‚îÇ  3. Fallback to LM Studio ‚Üí Success!                             ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  Logging: "Using LM Studio fallback (OpenAI, Ollama unavailable)"‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**UI: Workspace Override Settings:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Workspace Settings > AI Configuration                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ ‚öôÔ∏è Override Global AI Settings                                   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ ‚îå‚îÄ Provider Override ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ‚îÇ
‚îÇ ‚îÇ [ ] Use workspace-specific AI provider                        ‚îÇ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ‚îÇ
‚îÇ ‚îÇ     Provider: [Ollama (Local) ‚ñæ]                              ‚îÇ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ‚îÇ
‚îÇ ‚îÇ     ‚ìò Alle wiki pages en tasks in deze workspace              ‚îÇ‚îÇ
‚îÇ ‚îÇ       gebruiken Ollama i.p.v. OpenAI.                         ‚îÇ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ‚îÇ
‚îÇ ‚îÇ     Reden: [ Privacy - data blijft lokaal            ]        ‚îÇ‚îÇ
‚îÇ ‚îÇ            [ Offline werken                          ]        ‚îÇ‚îÇ
‚îÇ ‚îÇ            [ Kostenbesparing                         ]        ‚îÇ‚îÇ
‚îÇ ‚îÇ                                                               ‚îÇ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ [Cancel]                                            [Save]       ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

#### 14.5 Testing & Validation

> **Scope:** Test alle 3 providers en alle 3 capabilities.

| Item | Status | Notities |
|------|--------|----------|
| **Unit Tests** | | |
| OpenAiProvider unit tests | ‚úÖ | 28 tests - mock fetch responses |
| OllamaProvider unit tests | ‚úÖ | 7 tests - incl. native /api/tags fallback |
| LmStudioProvider unit tests | ‚úÖ | 3 tests - port 1234, OpenAI-compatible |
| Provider factory tests | ‚úÖ | 11 tests - createProvider(), createSimple*() |
| Error handling tests | ‚úÖ | 6 tests - auth failure, rate limit, network |
| Model categorization tests | ‚úÖ | 4 tests - embedding/reasoning/vision/unknown |
| **Integration Tests** | | |
| OpenAI embedding integration | ‚úÖ | Getest via test-ai-provider.ts script |
| OpenAI reasoning integration | ‚úÖ | Chat completion verified |
| Ollama embedding integration | ‚è∏Ô∏è | Local Ollama vereist (optioneel) |
| LM Studio embedding integration | ‚è∏Ô∏è | Local LM Studio vereist (optioneel) |
| **E2E Tests** | | |
| Provider switching in UI | ‚è∏Ô∏è | Deferred - unit tests voldoende voor v1 |
| Workspace override flow | ‚è∏Ô∏è | Deferred - handmatig getest via UI |

**Test file:** `apps/api/src/lib/ai/providers/__tests__/providers.test.ts` (60 tests, 100% pass)

**Test Matrix:**

| Provider | Embedding | Reasoning | Vision | Connection Test |
|----------|-----------|-----------|--------|-----------------|
| OpenAI | ‚úì text-embedding-3-small | ‚úì gpt-4o-mini | ‚úì gpt-4o | ‚úì /models |
| Ollama | ‚úì nomic-embed-text | ‚úì llama3.2 | ‚ö† llava | ‚úì /api/tags |
| LM Studio | ‚úì nomic-embed-text | ‚úì llama3.2 | ‚ö† llava | ‚úì /v1/models |

**CI/CD Configuratie:**

```yaml
# .github/workflows/ai-providers.yml
name: AI Provider Tests

on: [push, pull_request]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run provider unit tests
        run: pnpm test:providers

  integration-tests:
    runs-on: ubuntu-latest
    # Alleen met OpenAI (API key in secrets)
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
    steps:
      - uses: actions/checkout@v4
      - name: Run OpenAI integration tests
        run: pnpm test:providers:integration

  # Ollama/LM Studio tests draaien lokaal (self-hosted runner)
  local-integration:
    runs-on: self-hosted
    steps:
      - uses: actions/checkout@v4
      - name: Start Ollama
        run: ollama serve &
      - name: Run local provider tests
        run: pnpm test:providers:local
```

---

### Fase 14 Status Overzicht

| Sub-fase | Status | Beschrijving |
|----------|--------|--------------|
| **14.0 Research** | ‚úÖ **COMPLEET** | Alle 5 providers onderzocht, besluit genomen |
| ‚Ü≥ 14.0.1 Ollama | ‚úÖ | Hardware tiers, VRAM requirements, model selectie |
| ‚Ü≥ 14.0.2 Abacus.ai | ‚úÖ | ChatLLM Teams API ‚Üí **AFGEVALLEN** (geen embeddings) |
| ‚Ü≥ 14.0.3 OpenCode | ‚úÖ | Inspiratie voor provider abstractie |
| ‚Ü≥ 14.0.4 GLM-4.7 | ‚úÖ | Code specialist ‚Üí **AFGEVALLEN** (overkill voor Wiki) |
| ‚Ü≥ 14.0.5 LM Studio | ‚úÖ | GUI alternatief voor Ollama ‚Üí **GESELECTEERD** |
| **14.1 Database** | ‚úÖ | AiProviderConfig model + seed script |
| **14.2 Admin UI** | ‚úÖ | System Settings > AI Systems pagina |
| **14.3 Abstraction** | ‚úÖ | Provider interfaces + 3 implementaties |
| **14.4 Overrides** | ‚úÖ | Workspace level configuratie + override indicators |
| **14.5 Testing** | ‚úÖ | 60 unit tests + integration test script |

**Geselecteerde Providers:** OpenAI, Ollama, LM Studio
**Capabilities:** Embeddings, Reasoning, Vision (geen Code)

---

### Provider Support Matrix (Wiki/Graphiti Focus)

> **Scope:** Embeddings, Reasoning, Vision voor Wiki implementatie.
> **Niet in scope:** Code generation (niet relevant voor Wiki).

#### Geselecteerde Providers (v1)

| Provider | Embeddings | Reasoning | Vision | Deployment | Status |
|----------|------------|-----------|--------|------------|--------|
| **OpenAI** | ‚úÖ text-embedding-3-small | ‚úÖ gpt-4o-mini | ‚úÖ gpt-4o | Cloud | ‚úÖ Primair (Fase 10/11) |
| **Ollama** | ‚úÖ nomic-embed-text | ‚úÖ llama3.2 | ‚ö†Ô∏è¬π llava | Local | ‚úÖ Primair local |
| **LM Studio** | ‚úÖ nomic-embed-text | ‚úÖ llama3.2 | ‚ö†Ô∏è¬≤ | Local (GUI) | ‚úÖ Optioneel |

**Legenda:**
- ‚úÖ Volledig ondersteund
- ‚ö†Ô∏è Hardware-afhankelijk / beperkt

**Footnotes:**
1. **Ollama Vision:** Vereist 8+ GB VRAM, vision modellen moeten expliciet gepulled worden.
2. **LM Studio Vision:** Zelfde als Ollama, maar via GUI model browser.

---

#### Afgevallen Providers (Research Compleet)

| Provider | Reden Afvallen | Research Document |
|----------|----------------|-------------------|
| Anthropic | ‚ùå Geen embedding API | N/A |
| Abacus.ai | ‚ùå Embeddings alleen Enterprise ($5K+/maand) | [RESEARCH-Abacus-AI-ChatLLM.md](RESEARCH-Abacus-AI-ChatLLM.md) |
| GLM-4.7 | ‚ö†Ô∏è Code specialist, overkill voor Wiki | [RESEARCH-GLM-4.7.md](RESEARCH-GLM-4.7.md) |
| OpenCode | ‚ö†Ô∏è Inspiratie, geen provider | [RESEARCH-OpenCode.md](RESEARCH-OpenCode.md) |
| CUSTOM | ‚ö†Ô∏è Complexiteit niet nodig in v1 | N/A |

---

#### Provider Capabilities Detail

**OpenAI (Cloud - Primair):**
- **Embeddings:** text-embedding-3-small (1536 dim), text-embedding-3-large (3072 dim)
- **Reasoning:** gpt-4o-mini (goedkoop), gpt-4o (krachtig)
- **Vision:** gpt-4o met image input
- **API:** Standaard OpenAI SDK

**Ollama (Local - Primair):**
- **Embeddings:** nomic-embed-text (768 dim, 8192 context)
- **Reasoning:** llama3.2:8b, qwen3:8b, mistral:7b
- **Vision:** llava:7b (8GB+ VRAM vereist)
- **Minimum hardware:** 8GB VRAM (zie [RESEARCH-Ollama-Hardware.md](RESEARCH-Ollama-Hardware.md))
- **KRITIEK:** `num_ctx` moet naar 8192+ (default 2048 is te klein!)

**LM Studio (Local - Optioneel):**
- **Embeddings:** nomic-embed-text, bge-small
- **Reasoning:** Zelfde GGUF modellen als Ollama
- **Vision:** GGUF vision modellen
- **Voordelen:** GUI, Vulkan (integrated GPU), MLX (Apple Silicon)
- **Nadelen:** Geen echte headless, geen Docker, 20% langzamer
- **API:** OpenAI-compatible op poort 1234 (zie [RESEARCH-LM-Studio.md](RESEARCH-LM-Studio.md))

---

## Fase 15: Wiki Intelligence üÜï

> **Doel:** De volledige visie van "een wiki die zichzelf schrijft" realiseren door AI-powered search, Q&A, en enhanced graph visualization.
> **Afhankelijkheid:** Fase 14 (AI Providers) moet compleet zijn ‚úÖ
> **Drie Pijlers:** Semantic Search + Ask the Wiki + Enhanced Graphs

### Overzicht

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  FASE 15: Wiki Intelligence                                      ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ  ‚îÇ 15.2 Semantic ‚îÇ  ‚îÇ 15.3 Ask the  ‚îÇ  ‚îÇ 15.4 Enhanced         ‚îÇ‚îÇ
‚îÇ  ‚îÇ     Search    ‚îÇ  ‚îÇ     Wiki      ‚îÇ  ‚îÇ     Graphs            ‚îÇ‚îÇ
‚îÇ  ‚îÇ               ‚îÇ  ‚îÇ               ‚îÇ  ‚îÇ                       ‚îÇ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Embeddings  ‚îÇ  ‚îÇ ‚Ä¢ RAG Chat    ‚îÇ  ‚îÇ ‚Ä¢ Filtering           ‚îÇ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Hybrid      ‚îÇ  ‚îÇ ‚Ä¢ Sources     ‚îÇ  ‚îÇ ‚Ä¢ Clustering          ‚îÇ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ UI          ‚îÇ  ‚îÇ ‚Ä¢ History     ‚îÇ  ‚îÇ ‚Ä¢ Path finding        ‚îÇ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îÇ                            ‚îÇ                                     ‚îÇ
‚îÇ                    15.1 Provider Koppeling                       ‚îÇ
‚îÇ                    (Fase 14 ‚Üí Graphiti)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 15.1 Provider Koppeling (Foundation)

> **Doel:** Fase 14 AI Providers verbinden met Graphiti zodat workspace-specifieke configuratie wordt gebruikt.

**Probleem:** De Python Graphiti service gebruikt nu hardcoded `OPENAI_API_KEY` uit `.env`. Dit moet de geconfigureerde provider uit Fase 14 worden.

| Item | Status | Notities |
|------|--------|----------|
| **Backend Service** | | |
| WikiAiService class aanmaken | ‚ùå | `lib/ai/wiki/WikiAiService.ts` |
| getEmbeddingProvider(workspaceId) | ‚ùå | Haalt effective provider via registry |
| getReasoningProvider(workspaceId) | ‚ùå | Voor entity extraction, summarization |
| Provider caching per workspace | ‚ùå | Voorkom herhaalde DB lookups |
| **Graphiti Integratie** | | |
| graphitiService.ts updaten | ‚ùå | Inject provider i.p.v. hardcoded OpenAI |
| Embedding via provider | ‚ùå | `provider.embed()` i.p.v. Python service |
| Entity extraction via provider | ‚ùå | `provider.extractEntities()` |
| Fallback naar Python service | ‚ùå | Als Node provider faalt |
| **tRPC Endpoints** | | |
| wiki.getEffectiveProvider | ‚ùå | Toont welke provider actief is voor wiki |
| wiki.testProvider | ‚ùå | Test embedding + reasoning voor workspace |

**Architectuur:**

```typescript
// lib/ai/wiki/WikiAiService.ts
export class WikiAiService {
  constructor(private registry: ProviderRegistry) {}

  async getEmbeddings(workspaceId: number, texts: string[]): Promise<number[][]> {
    const provider = await this.registry.getEmbeddingProvider({ workspaceId })
    return provider.embedBatch(texts)
  }

  async extractEntities(workspaceId: number, text: string): Promise<Entity[]> {
    const provider = await this.registry.getReasoningProvider({ workspaceId })
    return provider.extractEntities(text, ['WikiPage', 'Task', 'User', 'Concept'])
  }
}
```

---

### 15.2 Semantic Search

> **Doel:** Zoeken op betekenis i.p.v. exacte keywords. "Find pages about authentication" vindt ook "OAuth2", "JWT", "Login flow".

| Item | Status | Notities |
|------|--------|----------|
| **Backend** | | |
| wiki.semanticSearch endpoint | ‚ùå | Query ‚Üí embedding ‚Üí vector search |
| Hybrid search (BM25 + vector) | ‚ùå | Combineer keyword + semantic |
| Search result ranking | ‚ùå | Score gebaseerd op relevantie |
| Cross-wiki search | ‚ùå | Zoek over workspace + project wiki's |
| Search caching | ‚ùå | Cache frequent queries |
| **Frontend** | | |
| WikiSemanticSearchDialog.tsx | ‚ùå | Nieuwe search dialog |
| Search mode toggle | ‚ùå | Text / Semantic / Hybrid switch |
| Search result preview | ‚ùå | Snippet met highlighted matches |
| "More like this" button | ‚ùå | Vind vergelijkbare pages |
| Recent searches | ‚ùå | Opslaan in localStorage |
| **Integratie** | | |
| Cmd+K semantic search | ‚ùå | CommandPalette integratie |
| WikiSidebar search | ‚ùå | Quick search in sidebar |

**Search Flow:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  User Query: "how does authentication work"                      ‚îÇ
‚îÇ                              ‚îÇ                                   ‚îÇ
‚îÇ                              ‚ñº                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ 1. Query ‚Üí Embedding (via Fase 14 provider)             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    "how does authentication work" ‚Üí [0.12, -0.34, ...]  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                              ‚îÇ                                   ‚îÇ
‚îÇ                              ‚ñº                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ 2. Vector Search (FalkorDB)                             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    Find nodes with similar embeddings                   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    + BM25 text search for keyword boost                 ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                              ‚îÇ                                   ‚îÇ
‚îÇ                              ‚ñº                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ 3. Results                                              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ Authentication Flow (0.92)                         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ OAuth2 Setup Guide (0.87)                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ JWT Token Management (0.84)                        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ Login Component (0.79)                             ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 15.3 Ask the Wiki (RAG Chat)

> **Doel:** Natural language Q&A over wiki content met bronvermelding. "Hoe werkt onze authenticatie?" ‚Üí Antwoord + bronnen.

| Item | Status | Notities |
|------|--------|----------|
| **RAG Pipeline** | | |
| Context retrieval | ‚ùå | Semantic search voor relevante chunks |
| Context ranking | ‚ùå | Top-K meest relevante passages |
| Context formatting | ‚ùå | Markdown chunks voor LLM |
| Prompt template | ‚ùå | System prompt met instructies |
| Answer generation | ‚ùå | Via reasoning provider |
| Source extraction | ‚ùå | Welke pages gebruikt voor antwoord |
| **Backend Endpoints** | | |
| wiki.askWiki | ‚ùå | Vraag stellen aan wiki |
| wiki.askWikiStream | ‚ùå | Streaming antwoord |
| wiki.getConversation | ‚ùå | Conversatie history ophalen |
| wiki.clearConversation | ‚ùå | History wissen |
| **Frontend Components** | | |
| AskWikiDialog.tsx | ‚ùå | Modal met chat interface |
| AskWikiPanel.tsx | ‚ùå | Sidebar panel variant |
| ChatMessage.tsx | ‚ùå | User/AI message bubbles |
| SourceCitation.tsx | ‚ùå | Klikbare bronvermelding |
| StreamingResponse.tsx | ‚ùå | Typing indicator + streaming |
| ConversationHistory.tsx | ‚ùå | Eerdere vragen tonen |
| **Features** | | |
| Follow-up questions | ‚ùå | Context behouden in gesprek |
| "Show me the source" | ‚ùå | Direct naar wiki page navigeren |
| Copy answer | ‚ùå | Kopieer naar clipboard |
| Feedback (üëç/üëé) | ‚ùå | Answer quality tracking |
| Scope selector | ‚ùå | Workspace / Project / Alles |

**UI Mockup:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîÆ Ask the Wiki                                          [√ó]   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  Scope: [Workspace: GenX ‚ñæ]                                     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ üë§ Hoe werkt onze authenticatie?                        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ ü§ñ Jullie applicatie gebruikt OAuth2 voor               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    authenticatie. Het proces werkt als volgt:           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    1. Gebruiker klikt op "Login with Google"            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    2. Na consent wordt een JWT token gegenereerd        ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    3. Token wordt opgeslagen voor sessie management     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    De implementatie is gedaan door @robin in sprint 23. ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    üìö Bronnen:                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ [Authentication Flow](wiki/auth-flow) ‚Üê            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ [Security Guidelines](wiki/security)               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    ‚Ä¢ [JWT Token Refresh](wiki/jwt-refresh)              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ    [üëç] [üëé] [üìã Copy]                                  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Stel een vraag...                               [Ask]   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  üí° Suggesties: "Hoe deploy ik naar productie?"                 ‚îÇ
‚îÇ                 "Wat zijn onze coding standards?"               ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**RAG Prompt Template:**

```typescript
const SYSTEM_PROMPT = `Je bent een behulpzame assistent die vragen beantwoordt
op basis van de wiki documentatie van het team.

REGELS:
1. Gebruik ALLEEN informatie uit de gegeven context
2. Als je het antwoord niet weet, zeg dat eerlijk
3. Citeer je bronnen met [Pagina Titel]
4. Antwoord in dezelfde taal als de vraag
5. Wees beknopt maar volledig

CONTEXT:
{context}

Beantwoord nu de vraag van de gebruiker.`
```

---

### 15.4 Enhanced Graphs

> **Doel:** De knowledge graph transformeren van simpele visualisatie naar een krachtig discovery tool.

| Item | Status | Notities |
|------|--------|----------|
| **Filtering & Controls** | | |
| Entity type filter | ‚ùå | Checkbox: WikiPage / Person / Concept / Task |
| Time range filter | ‚ùå | Slider: "Laatste week / maand / jaar / alles" |
| Depth control | ‚ùå | Hoeveel levels tonen (1-5) |
| Search within graph | ‚ùå | Highlight matching nodes |
| Hide/show orphans | ‚ùå | Nodes zonder connecties |
| **Clustering** | | |
| Auto-cluster detection | ‚ùå | Louvain / Label Propagation algoritme |
| Cluster coloring | ‚ùå | Elke cluster eigen kleur |
| Cluster labels | ‚ùå | Auto-generated cluster naam |
| Expand/collapse cluster | ‚ùå | Klik om cluster te openen |
| **Path Finding** | | |
| "How is X related to Y?" | ‚ùå | Shortest path tussen nodes |
| Path highlighting | ‚ùå | Animatie langs het pad |
| Path explanation | ‚ùå | "X ‚Üí linked to ‚Üí Y ‚Üí mentions ‚Üí Z" |
| **Node Details** | | |
| Hover card | ‚ùå | Quick preview bij hover |
| Detail panel | ‚ùå | Sidebar met volledige info |
| Node connections list | ‚ùå | Alle edges van/naar node |
| Quick actions | ‚ùå | Open / Edit / Find related |
| **Advanced Visualization** | | |
| Mini-map | ‚ùå | Overzicht in hoek |
| Zoom to fit | ‚ùå | Automatisch schalen |
| Layout options | ‚ùå | Force / Hierarchical / Radial |
| Timeline mode | ‚ùå | Nodes op tijdlijn (created_at) |
| **Export & Sharing** | | |
| Export PNG | ‚ùå | Screenshot van graph |
| Export SVG | ‚ùå | Vector voor print |
| Export JSON | ‚ùå | Graph data voor externe tools |
| Share view | ‚ùå | URL met filters/positie |

**Enhanced Graph UI:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üåê Knowledge Graph: GenX Workspace                       [√ó]   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                  ‚îÇ üìã Filters                   ‚îÇ
‚îÇ                                  ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ ‚îÇ ‚òë WikiPage             ‚îÇ   ‚îÇ
‚îÇ      ‚ï± Auth ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ ‚îÇ ‚òë Person               ‚îÇ   ‚îÇ
‚îÇ     ‚ï± Cluster‚ï≤     ‚îÇ             ‚îÇ ‚îÇ ‚òë Concept              ‚îÇ   ‚îÇ
‚îÇ    ‚óØ OAuth2  ‚óØ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îê          ‚îÇ ‚îÇ ‚òê Task                 ‚îÇ   ‚îÇ
‚îÇ     ‚ï≤ JWT   ‚ï±      ‚îÇ  ‚îÇ          ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ      ‚ï≤_____‚ï±       ‚îÇ  ‚îÇ          ‚îÇ                              ‚îÇ
‚îÇ         ‚îÇ          ‚îÇ  ‚îÇ          ‚îÇ üîç Search in graph           ‚îÇ
‚îÇ         ‚ñº          ‚îÇ  ‚îÇ          ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ    ‚óØ @robin ‚óØ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ          ‚îÇ ‚îÇ authentication...      ‚îÇ   ‚îÇ
‚îÇ                       ‚îÇ          ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ          ‚îÇ                              ‚îÇ
‚îÇ      ‚ï± API  ‚ï≤‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ üìä Stats                     ‚îÇ
‚îÇ     ‚ï±Cluster‚ï≤                    ‚îÇ ‚Ä¢ 47 nodes                   ‚îÇ
‚îÇ    ‚óØ REST   ‚óØ                    ‚îÇ ‚Ä¢ 123 edges                  ‚îÇ
‚îÇ     ‚ï≤GraphQL‚ï±                    ‚îÇ ‚Ä¢ 4 clusters                 ‚îÇ
‚îÇ      ‚ï≤_____‚ï±                     ‚îÇ                              ‚îÇ
‚îÇ                                  ‚îÇ ‚ö° Actions                   ‚îÇ
‚îÇ  [Mini-map]                      ‚îÇ [Path: A ‚Üí B]               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ [Export PNG]                ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢  ‚îÇ                          ‚îÇ [Export JSON]               ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layout: [Force ‚ñæ]  Depth: [3 ‚ñæ]  Time: [All ‚ñæ]   [Fit] [Reset] ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Path Finding UI:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üîó Path Finder                                                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  From: [OAuth2           ‚ñæ]                                     ‚îÇ
‚îÇ  To:   [@robin           ‚ñæ]                                     ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  OAuth2 ‚îÄ‚îÄmentions‚îÄ‚îÄ‚ñ∂ Authentication Flow ‚îÄ‚îÄauthor‚îÄ‚îÄ‚ñ∂ @robin‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  Path length: 2 hops                                     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                          ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  [Highlight in Graph] [Show Alternative Paths]                  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 15.5 Integration & Polish

> **Doel:** Alle componenten integreren in een coherente gebruikerservaring.

| Item | Status | Notities |
|------|--------|----------|
| **UI Integration** | | |
| WikiPageView integratie | ‚ùå | Ask Wiki button in toolbar |
| WikiSidebar integratie | ‚ùå | Search + Graph + Ask toggles |
| Keyboard shortcuts | ‚ùå | Cmd+Shift+A = Ask Wiki |
| Context menu | ‚ùå | Rechtermuisklik ‚Üí "Ask about this" |
| **Cross-Feature Links** | | |
| Search ‚Üí Graph | ‚ùå | "Show in graph" button |
| Graph ‚Üí Ask | ‚ùå | "Ask about this node" |
| Ask ‚Üí Sources ‚Üí Page | ‚ùå | Klikbare bronnen |
| **Performance** | | |
| Embedding caching | ‚ùå | Cache wiki page embeddings |
| Lazy loading graph | ‚ùå | Load nodes on demand |
| Debounced search | ‚ùå | Wacht tot user stopt typen |
| Background indexing | ‚ùå | Re-index bij idle |
| **Analytics** | | |
| Search analytics | ‚ùå | Wat zoeken users? |
| Ask Wiki analytics | ‚ùå | Welke vragen worden gesteld? |
| Graph usage | ‚ùå | Welke filters populair? |
| Answer quality | ‚ùå | üëç/üëé aggregatie |
| **Testing** | | |
| Unit tests | ‚ùå | WikiAiService tests |
| Integration tests | ‚ùå | RAG pipeline tests |
| E2E tests | ‚ùå | Full flow tests |

---

### 15.6 Status Overzicht

| Sub-fase | Status | Beschrijving |
|----------|--------|--------------|
| **15.1 Provider Koppeling** | ‚ùå | Fase 14 ‚Üí Graphiti bridge |
| **15.2 Semantic Search** | ‚ùå | Zoeken op betekenis |
| **15.3 Ask the Wiki** | ‚ùå | RAG Chat met bronnen |
| **15.4 Enhanced Graphs** | ‚ùå | Filtering, clustering, paths |
| **15.5 Integration** | ‚ùå | UI polish en performance |

**Totaal items:** ~60 taken verdeeld over 5 sub-fases

---

### Aanbevolen Volgorde

```
15.1 Provider Koppeling  ‚îÄ‚îÄ‚îê
                           ‚îú‚îÄ‚îÄ‚ñ∂ 15.2 Semantic Search ‚îÄ‚îÄ‚îê
                           ‚îÇ                           ‚îÇ
                           ‚îî‚îÄ‚îÄ‚ñ∂ 15.4 Enhanced Graphs ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∂ 15.5 Integration
                                                       ‚îÇ
                               15.3 Ask the Wiki ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

1. **15.1 eerst** - Fundament voor alles
2. **15.2 en 15.4 parallel** - Onafhankelijk van elkaar
3. **15.3 na 15.2** - RAG heeft semantic search nodig
4. **15.5 laatste** - Alles samenvoegen

---

## Graphiti Architectuur

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Kanbu Stack                              ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Web App    ‚îÇ  ‚îÇ   API (Node) ‚îÇ  ‚îÇ   Graphiti (Python)    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   React      ‚îÇ‚îÄ‚îÄ‚îÇ   Fastify    ‚îÇ‚îÄ‚îÄ‚îÇ   FastAPI              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   Vite       ‚îÇ  ‚îÇ   tRPC       ‚îÇ  ‚îÇ   graphiti_core        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   :5173      ‚îÇ  ‚îÇ   :3001      ‚îÇ  ‚îÇ   :8000                ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                              ‚îÇ                    ‚îÇ              ‚îÇ
‚îÇ                              ‚ñº                    ‚ñº              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                     Data Layer                            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ PostgreSQL   ‚îÇ  ‚îÇ FalkorDB     ‚îÇ  ‚îÇ Qdrant         ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ :5432        ‚îÇ  ‚îÇ :6379        ‚îÇ  ‚îÇ :6333          ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Source data  ‚îÇ  ‚îÇ Graph DB     ‚îÇ  ‚îÇ Vectors        ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                     LLM Layer                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ OpenAI       ‚îÇ  ‚îÇ Anthropic    ‚îÇ  ‚îÇ Ollama (local) ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ gpt-4o-mini  ‚îÇ  ‚îÇ claude-3     ‚îÇ  ‚îÇ llama3.2       ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Blocking Issues

| Issue | Impact | Oplossing |
|-------|--------|-----------|
| ~~Geen Graphiti server~~ | ~~Fase 2 blocked~~ | ‚úÖ FalkorDB draait |
| Project Wiki page mist | Minor | Kan later, workspace wiki werkt |
| ~~tRPC endpoints voor graph queries~~ | ~~Fase 3 blocked~~ | ‚úÖ graphiti.ts router |

---

## Quick Commands

```bash
# Start Kanbu dev
bash ~/genx/v6/dev/kanbu/scripts/api.sh start
cd ~/genx/v6/dev/kanbu/apps/web && pnpm dev

# Wiki URL
https://max:5173/workspace/genx/wiki

# Graphiti docs
cat ~/genx/v6/dev/kanbu/docs/WIKI-base/GRAPHITI-IMPLEMENTATIE.md
```

---

## Changelog

| Datum | Actie |
|-------|-------|
| 2026-01-12 | Fase 0 & 1 compleet, MarkdownPastePlugin met Showdown |
| 2026-01-12 | Roadmap bestand aangemaakt |
| 2026-01-12 | FalkorDB toegevoegd aan docker-compose.yml |
| 2026-01-12 | GraphitiService.ts aangemaakt |
| 2026-01-12 | Sync hooks toegevoegd aan workspaceWiki.ts en projectWiki.ts |
| 2026-01-12 | graphiti.ts tRPC router toegevoegd (Fase 2 compleet) |
| 2026-01-12 | BacklinksPanel.tsx component aangemaakt |
| 2026-01-12 | BacklinksPanel ge√Øntegreerd in WikiPageView.tsx |
| 2026-01-12 | TaskRefNode.tsx en TaskRefPlugin.tsx toegevoegd (#task-refs) |
| 2026-01-12 | MentionNode.tsx en MentionPlugin.tsx toegevoegd (@mentions) |
| 2026-01-12 | SignatureNode.tsx en SignaturePlugin.tsx toegevoegd (&Sign) |
| 2026-01-12 | Dropdown positioning fix (center ipv far right) |
| 2026-01-12 | WikiLinkNode importJSON fix voor duplicate children bug |
| 2026-01-12 | Fase 3 COMPLEET |
| 2026-01-12 | WikiSearchDialog.tsx met local + semantic search |
| 2026-01-12 | Search dialog ge√Øntegreerd in WorkspaceWikiPage |
| 2026-01-12 | Wiki pages zoeken via Cmd+K CommandPalette |
| 2026-01-12 | Fase 4 COMPLEET |
| 2026-01-12 | D3.js ge√Ønstalleerd voor graph visualization |
| 2026-01-12 | getGraph endpoint toegevoegd aan graphiti router |
| 2026-01-12 | WikiGraphView.tsx component met D3.js force-directed graph |
| 2026-01-12 | Graph toggle button in WikiSidebar |
| 2026-01-12 | Correctie: "Semantic search" ‚Üí "Text search" (geen echte vectors) |
| 2026-01-12 | GRAPHITI CORE INTEGRATIE roadmap toegevoegd (Fase 7-13) |
| 2026-01-12 | apps/graphiti/ directory + graphiti_core gekopieerd |
| 2026-01-12 | pyproject.toml + FastAPI service (main.py, schemas.py) |
| 2026-01-12 | Dockerfile + .env.example aangemaakt |
| 2026-01-12 | graphiti service toegevoegd aan docker-compose.yml |
| 2026-01-12 | Fase 7 COMPLEET |
| 2026-01-12 | GraphitiClient class (lib/graphitiClient.ts) |
| 2026-01-12 | graphitiService.ts v2 - Python service + fallback |
| 2026-01-12 | temporalSearch() method toegevoegd |
| 2026-01-12 | GRAPHITI_SERVICE_URL in .env |
| 2026-01-12 | Fase 8 COMPLEET |
| 2026-01-12 | Fase 9 gestart: Bi-Temporal Model |
| 2026-01-12 | graphiti_core heeft native valid_at/invalid_at support (9.1 ‚úÖ) |
| 2026-01-12 | temporalSearch tRPC endpoint toegevoegd aan graphiti.ts (9.3 ‚úÖ) |
| 2026-01-12 | POST /search/temporal endpoint in Python service |
| 2026-01-12 | WikiTemporalSearch.tsx component aangemaakt (9.4 ‚úÖ) |
| 2026-01-12 | Temporal search button toegevoegd aan WikiSidebar (Clock icon) |
| 2026-01-12 | **Fase 10 gestart: LLM Entity Extraction** |
| 2026-01-12 | Custom entity types aangemaakt: WikiPage, Task, User, Project, Concept |
| 2026-01-12 | src/entity_types/kanbu_entities.py met Pydantic models |
| 2026-01-12 | AddEpisodeRequest uitgebreid met use_kanbu_entities optie |
| 2026-01-12 | /entity-types endpoint toegevoegd aan Python service |
| 2026-01-12 | graphitiService.ts gebruikt nu Kanbu entity types by default |
| 2026-01-12 | Entity details in AddEpisodeResponse (entity_name, entity_type) |
| 2026-01-12 | Fase 10.1-10.3 COMPLEET |
| 2026-01-12 | **Fase 10 COMPLEET** - Relation extraction en deduplicatie zijn native in graphiti_core |
| 2026-01-12 | **Fase 11 gestart: Embeddings & Semantic Search** |
| 2026-01-12 | OpenAI embedder expliciet geconfigureerd in main.py |
| 2026-01-12 | Environment variabelen: EMBEDDING_MODEL, EMBEDDING_DIM |
| 2026-01-12 | HybridSearchRequest/Response schemas toegevoegd |
| 2026-01-12 | POST /search/hybrid endpoint met BM25 + vector + BFS |
| 2026-01-12 | Reranking support: RRF, MMR, Cross-encoder |
| 2026-01-12 | TypeScript client uitgebreid met hybridSearch() method |
| 2026-01-12 | HealthResponse uitgebreid met embedding_model en embedding_dim |
| 2026-01-12 | **Fase 11 COMPLEET** |
| 2026-01-12 | **Fase 14 toegevoegd: AI Provider Configuration** |
| 2026-01-12 | Sub-fases: 14.0 Research, 14.1 Database, 14.2 Admin UI, 14.3 Abstraction, 14.4 Overrides, 14.5 Testing |
| 2026-01-12 | Provider Support Matrix: OpenAI, Anthropic (placeholder embeddings), Ollama, Abacus.ai |
| 2026-01-12 | 3-level configuratie: Global ‚Üí Workspace ‚Üí Project inheritance |
| 2026-01-12 | **14.0.1 Ollama Research toegevoegd:** Hardware tiers, VRAM matrices, model selectie |
| 2026-01-12 | Open vragen: Minimum hardware, GPU auto-detection, cloud fallback strategie |
| 2026-01-12 | Development hardware gedocumenteerd: AMD RYZEN AI MAX+ 395 / Radeon 8060S / 123GB RAM |
| 2026-01-12 | Multi-vendor support vragen: NVIDIA CUDA vs AMD ROCm vs Apple Metal |
| 2026-01-12 | **NPU tier toegevoegd:** AMD XDNA (`/dev/accel0`) aanwezig op MAX - 50 TOPS |
| 2026-01-12 | Open vragen: NPU vs GPU performance, Ryzen AI software stack |
| 2026-01-12 | **14.0.2 Abacus.ai Research COMPLEET** - Zie RESEARCH-Abacus-AI-ChatLLM.md |
| 2026-01-12 | Conclusie 14.0.2: Niet aanbevolen als primaire provider (geen embeddings in Teams tier) |
| 2026-01-12 | **14.0.3 OpenCode toegevoegd:** Open source AI coding agent, 75+ providers, self-hosted support |
| 2026-01-12 | **14.0.4 GLM-4.7 toegevoegd:** Z.ai Chinese open-source model, #1 SWE-bench |
| 2026-01-12 | Provider Support Matrix uitgebreid met OpenCode en GLM-4.7 |
| 2026-01-12 | **14.0.4 GLM-4.7 Research COMPLEET** - Zie RESEARCH-GLM-4.7.md |
| 2026-01-12 | GLM-4.7: 358B MoE model (GEEN 9B/32B varianten!), $0.40/$1.50 per 1M tokens |
| 2026-01-12 | GLM-4.7 self-hosted: 135-205GB RAM vereist - niet haalbaar voor community |
| 2026-01-12 | Z.ai embedding-3 model beschikbaar voor vector embeddings |
| 2026-01-12 | Conclusie 14.0.4: ‚úÖ Aanbevolen als CODE provider via API, ‚ùå niet voor self-hosted |
| 2026-01-12 | **14.0.3 OpenCode Research COMPLEET** - Zie RESEARCH-OpenCode.md |
| 2026-01-12 | OpenCode: 50K+ stars, 75+ providers via AI SDK + Models.dev |
| 2026-01-12 | OpenCode Zen: Pay-as-you-go gateway met free tier (GLM-4.7, Grok Code) |
| 2026-01-12 | KRITIEK: Ollama default num_ctx = 4096 - te klein voor agents! Fix nodig |
| 2026-01-12 | Conclusie 14.0.3: Inspiratie voor provider abstractie, geen directe integratie (geen embeddings) |
| 2026-01-12 | **14.0.1 Ollama Hardware Research COMPLEET** - Zie RESEARCH-Ollama-Hardware.md |
| 2026-01-12 | Hardware tiers gedefinieerd: CPU-only, Entry (8GB), Mid (12GB), High (24GB), Pro (48GB+), Apple Silicon |
| 2026-01-12 | VRAM matrices: LLM modellen (7B-70B), Embedding modellen, Vision modellen |
| 2026-01-12 | Quantization impact: Q4_K_M = best balance (4x minder dan FP16) |
| 2026-01-12 | GPU vendor support: NVIDIA ‚úÖ, AMD ROCm ‚úÖ, Apple Metal ‚úÖ, Intel Vulkan ‚ö†Ô∏è |
| 2026-01-12 | NPU support: ‚ùå Niet in Ollama/llama.cpp (AMD XDNA, Intel NPU niet bruikbaar) |
| 2026-01-12 | KRITIEK: Ollama default num_ctx = 2048 - moet naar 8192+ voor Kanbu! |
| 2026-01-12 | Kanbu minimum: 8GB VRAM (llama3.2:8b + nomic-embed-text) |
| 2026-01-12 | **Fase 14.0.5 LM Studio Research COMPLEET** - Zie RESEARCH-LM-Studio.md |
| 2026-01-12 | LM Studio: Desktop app + CLI, OpenAI-compatible API, GGUF + MLX engines |
| 2026-01-12 | LM Studio vs Ollama: Ollama 20% sneller, LM Studio beter op integrated GPU (Vulkan) |
| 2026-01-12 | LM Studio beperkingen: Geen echte headless, geen Docker, closed source |
| 2026-01-12 | Conclusie 14.0.5: Optioneel alternatief voor GUI/desktop users, niet voor servers |
| 2026-01-12 | **Fase 14.0 Research volledig COMPLEET** - Alle 5 sub-fases afgerond |
| 2026-01-12 | **BESLUIT: Provider selectie voor Wiki/Graphiti** |
| 2026-01-12 | Geselecteerd: OpenAI (cloud), Ollama (local primair), LM Studio (local optioneel) |
| 2026-01-12 | Afgevallen: Anthropic (geen embeddings), Abacus.ai (te duur), GLM-4.7 (overkill), CUSTOM (v1 scope) |
| 2026-01-12 | AiProviderType enum: OPENAI, OLLAMA, LM_STUDIO |
| 2026-01-12 | AiCapability enum: EMBEDDING, REASONING, VISION (geen CODE - niet voor Wiki) |
| 2026-01-12 | Provider Support Matrix ge√ºpdatet met Wiki focus |
| 2026-01-12 | **Fase 14.2-14.5 aangepast aan nieuwe provider selectie** |
| 2026-01-12 | 14.2 Admin UI: 3 providers (OpenAI, Ollama, LM Studio), nieuwe UI mockup |
| 2026-01-12 | 14.3 Abstraction: EmbeddingProvider, ReasoningProvider, VisionProvider interfaces |
| 2026-01-12 | 14.3: Anthropic/Abacus.ai providers verwijderd uit scope |
| 2026-01-12 | 14.4 Overrides: Fallback chain OpenAI ‚Üí Ollama ‚Üí LM Studio |
| 2026-01-12 | 14.5 Testing: Test matrix voor 3 providers, CI/CD configuratie |
| 2026-01-12 | **Fase 14.1 Database Model COMPLEET** |
| 2026-01-12 | AiProviderType enum toegevoegd aan schema.prisma (OPENAI, OLLAMA, LM_STUDIO) |
| 2026-01-12 | AiCapability enum toegevoegd (EMBEDDING, REASONING, VISION) |
| 2026-01-12 | AiProviderConfig model toegevoegd met alle velden |
| 2026-01-12 | Relations toegevoegd aan Workspace, Project, User models |
| 2026-01-12 | `pnpm prisma db push` succesvol uitgevoerd |
| 2026-01-12 | seed-ai-providers.ts script aangemaakt |
| 2026-01-12 | 3 global providers geseeded: OpenAI, Ollama, LM Studio (alle inactief) |
| 2026-01-12 | **Fase 14.2 Admin UI COMPLEET** |
| 2026-01-12 | aiProvider.ts tRPC router toegevoegd (CRUD + testConnection + getModels) |
| 2026-01-12 | AiSystemsPage.tsx admin pagina aangemaakt |
| 2026-01-12 | ProviderCard component met test functionaliteit |
| 2026-01-12 | Create/Edit modals voor provider configuratie |
| 2026-01-12 | AI Systems menu item toegevoegd aan AdminSidebar |
| 2026-01-12 | Route /admin/settings/ai toegevoegd aan App.tsx |
| 2026-01-12 | **Fase 14.3 Provider Abstraction Layer COMPLEET** |
| 2026-01-12 | types.ts met AiProvider, EmbeddingProvider, ReasoningProvider, VisionProvider interfaces |
| 2026-01-12 | OpenAiCompatibleProvider base class voor gedeelde OpenAI-compatible functionaliteit |
| 2026-01-12 | OpenAiProvider implementatie (cloud, alle capabilities) |
| 2026-01-12 | OllamaProvider implementatie (local, met native /api/tags fallback) |
| 2026-01-12 | LmStudioProvider implementatie (local/desktop GUI) |
| 2026-01-12 | factory.ts met createProvider(), createEmbeddingProvider(), etc. |
| 2026-01-12 | registry.ts met ProviderRegistry singleton en scope resolution |
| 2026-01-12 | Barrel export via lib/ai/providers/index.ts |
| 2026-01-12 | **Fase 14.4 Workspace & Project Overrides COMPLEET** |
| 2026-01-12 | workspaceAiProvider.ts tRPC router (CRUD voor workspace admins) |
| 2026-01-12 | getEffectiveAll endpoint voor capability overview |
| 2026-01-12 | WorkspaceAiConfigCard.tsx component met override indicators |
| 2026-01-12 | Ge√Øntegreerd in WorkspaceSettingsPage.tsx |
| 2026-01-12 | Inheritance: Project > Workspace > Global (via registry.findEffectiveConfig) |
| 2026-01-12 | Fallback: registry.getProviderWithFallback() voor automatic failover |
| 2026-01-12 | Project level override: ‚è∏Ô∏è Deferred - workspace level voldoende voor v1 |
| 2026-01-12 | **Fase 14.5 Testing & Validation COMPLEET** |
| 2026-01-12 | providers.test.ts met 60 unit tests (Vitest + mocked fetch) |
| 2026-01-12 | OpenAiProvider tests: constructor, connection, embed, chat, vision (28 tests) |
| 2026-01-12 | OllamaProvider tests: native /api/tags fallback, default models (7 tests) |
| 2026-01-12 | LmStudioProvider tests: port 1234, OpenAI-compatible (3 tests) |
| 2026-01-12 | Factory tests: createProvider, createSimple*, getDefaultUrl, requiresApiKey (11 tests) |
| 2026-01-12 | Error handling tests: auth failure, rate limit, network errors (6 tests) |
| 2026-01-12 | Model categorization tests: embedding/reasoning/vision/unknown (4 tests) |
| 2026-01-12 | test-ai-provider.ts script voor handmatige integration tests |
| 2026-01-12 | OpenAI live test: Connection ‚úÖ (648ms), Embedding ‚úÖ (1536 dim), Reasoning ‚úÖ |
| 2026-01-12 | **Fase 14 AI Provider Configuration VOLLEDIG COMPLEET** |
| 2026-01-12 | **Fase 15 Wiki Intelligence toegevoegd** - Semantic Search + Ask the Wiki + Enhanced Graphs |
